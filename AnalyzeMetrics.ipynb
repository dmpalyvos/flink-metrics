{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from matplotlib import lines, markers\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLINK_ENDPOINT = 'http://localhost:8081'\n",
    "SAMPLING_FREQ_SEC = 1\n",
    "ANALYSIS_DURATION_SEC = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitComponent(component, pattern):\n",
    "    m = pattern.match(component)\n",
    "    if m:\n",
    "        mdict = matchDict(m)\n",
    "        return mdict['instance'], mdict['component'], mdict['metric']\n",
    "    raise Exception(f'Failed to match {component}!')\n",
    "    \n",
    "def getTaskManagers():\n",
    "    return requests.get(f'{FLINK_ENDPOINT}/taskmanagers').json()['taskmanagers']\n",
    "    \n",
    "def getAvailableTaskManagerMetrics():\n",
    "    return [metric['id'] for metric in requests.get(f'{FLINK_ENDPOINT}/taskmanagers/metrics').json()]\n",
    "\n",
    "def getTaskManagerMetrics(tmID, metrics):\n",
    "    metricString = ','.join(metrics)\n",
    "    return requests.get(f'{FLINK_ENDPOINT}/taskmanagers/{tmID}/metrics', params={'get': metricString}).json()\n",
    "    \n",
    "def getAvailableVertexMetrics(jobID, vertexID):\n",
    "    return [metric['id'] for metric in requests.get(f'{FLINK_ENDPOINT}/jobs/{jobID}/vertices/{vertexID}/metrics').json()]\n",
    "\n",
    "def getJobInfo(jobID):\n",
    "    return requests.get(f'{FLINK_ENDPOINT}/jobs/{jobID}').json()\n",
    "\n",
    "def getAvailableJobMetrics(jobID):\n",
    "    jobMetrics = dict() # vertexID -> [metrics]\n",
    "    vertexes = getJobInfo(jobID)['vertices']\n",
    "    for vertex in vertexes:\n",
    "        jobMetrics[vertex['id']] = getAvailableVertexMetrics(jobID, vertex['id'])\n",
    "    return jobMetrics\n",
    "\n",
    "def getJobMetricNames(jobMetrics, namePattern):\n",
    "    metricNames = set()\n",
    "    for metrics in jobMetrics.values():\n",
    "        for metric in metrics:\n",
    "            m = namePattern.match(metric)\n",
    "            if m:\n",
    "                metricNames.add(m.group('metric'))\n",
    "    return metricNames\n",
    "\n",
    "\n",
    "def updateVertexRequests(selectedMetrics, availableJobMetrics, namePattern, vertexRequests):\n",
    "    \"\"\"Given a list of metrics names, update the given vertex requests dictonary\n",
    "\n",
    "    Keyword arguments:\n",
    "    selectedMetrics -- A list of metric names \n",
    "    availableJobMetrics -- A dictionary of vertexID: [metrics], as returned by getAvailableJobMetrics\n",
    "    namePattern -- A regular expression that converts metricName -> metric (and optionally filters unwanted metrics)\n",
    "    vertexRequests -- A dictionary of vertexID: [metrics] that will be used for recording statistics\n",
    "    \"\"\"\n",
    "    for vertexID, availableMetrics in availableJobMetrics.items():\n",
    "        filteredMetrics = []\n",
    "        for metric in availableMetrics:\n",
    "            m = namePattern.match(metric)\n",
    "            if m and m.group('metric') in selectedMetrics:\n",
    "                filteredMetrics.append(metric)\n",
    "        vertexRequests[vertexID] = filteredMetrics\n",
    "        print(f'{len(vertexRequests[vertexID])} metrics for {vertexID}')\n",
    "        \n",
    "def selectVertexMetrics(availableJobMetrics, vertexRequests, namePattern, description):\n",
    "    widget = widgets.SelectMultiple(description=description, options=getJobMetricNames(availableJobMetrics, namePattern))\n",
    "    interact(updateVertexRequests, selectedMetrics=widget, namePattern=fixed(namePattern), \n",
    "             availableJobMetrics=fixed(availableJobMetrics), vertexRequests=fixed(vertexRequests))\n",
    "\n",
    "def getVertexMetrics(jobID, vertexID, metrics, maxRequestLength=40):\n",
    "    def rawGetJobMetrics(jobID, vertexID, metrics):\n",
    "        metricString = ','.join(metrics)\n",
    "        return requests.get(f'{FLINK_ENDPOINT}/jobs/{jobID}/vertices/{vertexID}/metrics', params={'get': metricString}).json()\n",
    "    completeJSON = []\n",
    "    # Split metric requests so that the request string does not become too long\n",
    "    for i in range(0, len(metrics), maxRequestLength):\n",
    "        partialMetrics = metrics[i:i+maxRequestLength]\n",
    "        completeJSON += rawGetJobMetrics(jobID, vertexID, partialMetrics)\n",
    "    return completeJSON\n",
    "\n",
    "def matchDict(match):\n",
    "    d = defaultdict(lambda: 'DEFAULT')\n",
    "    matchDict = match.groupdict()\n",
    "    d.update(matchDict)\n",
    "    return d\n",
    "\n",
    "def plotAggregated(df, ax, startTime, aggregateFor, groupBy):\n",
    "    markerstyles = list(markers.MarkerStyle.markers.keys())\n",
    "    aggregated = df.groupby(aggregateFor).aggregate({'value': [np.mean, np.std]})\n",
    "    for i, (name, group) in enumerate(aggregated.groupby(level=groupBy)):\n",
    "        data = group.reset_index()\n",
    "        data.t -= startTime\n",
    "        ax.plot(data.t, data.value['mean'], alpha=.7, label=name[0][:5] + '_' + name[1][:15], \n",
    "                marker=markerstyles[i % len(markerstyles)], markevery=10, markersize=5)\n",
    "        ax.fill_between(data.t, data.value['mean'] - data.value['std']/2, data.value['mean'] + data.value['std']/2, alpha=.3)\n",
    "        \n",
    "def recordVertexMetrics(df, vertexMetrics, timestamp, namePattern):\n",
    "    for vertexID, metrics in vertexMetrics.items():\n",
    "        metricValues = getVertexMetrics(jobID, vertexID, metrics)\n",
    "        for metric in metricValues:\n",
    "            componentInstance, componentName, baseMetric  = splitComponent(metric['id'], namePattern)\n",
    "            df = df.append({'t': int(timestamp), \n",
    "                                      'vertex': vertexID, \n",
    "                                      'component': componentName, \n",
    "                                      'instance': componentInstance, \n",
    "                                      'metric': baseMetric, \n",
    "                                      'value': float(metric['value'])}, \n",
    "                                       ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def recordTaskManagerMetrics(df, taskManagers, taskManagerMetrics, timestamp):\n",
    "    for tm in taskManagers:\n",
    "        metricValues = getTaskManagerMetrics(tm['id'], taskManagerMetrics)\n",
    "        for metric in metricValues:\n",
    "            df = df.append({'t': int(timestamp), \n",
    "                            'tm': tm['id'], \n",
    "                            'metric': metric['id'], \n",
    "                            'value': float(metric['value'])}, \n",
    "                           ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get running job\n",
    "# TODO: Report for multiple jobs\n",
    "jobs = requests.get(f'{FLINK_ENDPOINT}/jobs').json()['jobs']\n",
    "taskManagers = getTaskManagers()\n",
    "runningJobs = [job for job in jobs if job['status'] == 'RUNNING']\n",
    "assert len(runningJobs) == 1, 'Toolkit can only work with exactly one running job!'\n",
    "jobID = runningJobs[0]['id']\n",
    "print(f'Reporting for job \"{jobID}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobData = pd.DataFrame(columns=['t', 'vertex', 'component', 'instance', 'metric', 'value'])\n",
    "jobData['t'] = jobData['t'].astype(int)\n",
    "jobData['value'] = jobData['value'].astype(float)\n",
    "\n",
    "tmData = pd.DataFrame(columns=['t', 'tm', 'metric', 'value'])\n",
    "tmData['t'] = tmData['t'].astype(int)\n",
    "tmData['value'] = tmData['value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex patterns for splitting metric names\n",
    "OPERATOR_METRIC_PATTERN = re.compile('^(?P<instance>\\d+)\\.(?P<component>.+)\\.(?P<metric>.+)?$')\n",
    "TASK_METRIC_PATTERN = re.compile('^(?P<instance>\\d+)\\.(?P<metric>[^\\.]+)$')\n",
    "\n",
    "# Task and operator metrics need to be requested from specific vertexes\n",
    "taskMetrics = dict()\n",
    "operatorMetrics = dict()\n",
    "availableJobMetrics = getAvailableJobMetrics(jobID)\n",
    "selectVertexMetrics(availableJobMetrics, taskMetrics, TASK_METRIC_PATTERN, 'task metrics')\n",
    "selectVertexMetrics(availableJobMetrics, operatorMetrics, OPERATOR_METRIC_PATTERN, 'op metrics')\n",
    "\n",
    "# Task Manager Metrics are requested from all taskmanagers\n",
    "# so we just maintain the names\n",
    "taskManagerMetrics = None\n",
    "@interact(selectedMetrics=widgets.SelectMultiple(description='tm metrics', options=getAvailableTaskManagerMetrics()))\n",
    "def selectTaskManagerMetrics(selectedMetrics):\n",
    "    global taskManagerMetrics\n",
    "    taskManagerMetrics = selectedMetrics if selectedMetrics else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "def plotMetric(df, ax, startTime, metric, aggregateFor, groupBy):\n",
    "    ax.clear()\n",
    "    plotAggregated(df[df.metric == metric], ax, startTime, aggregateFor, groupBy)\n",
    "    ax.legend()\n",
    "    ax.set(xlabel='sec', title=metric)    \n",
    "\n",
    "taskMetricNames = list(getJobMetricNames(taskMetrics, TASK_METRIC_PATTERN))\n",
    "operatorMetricNames = list(getJobMetricNames(operatorMetrics, OPERATOR_METRIC_PATTERN))\n",
    "nPlots = len(taskManagerMetrics) + len(operatorMetricNames) + len(taskMetricNames)\n",
    "fig, axes = plt.subplots(figsize=(8, 4*nPlots), nrows=nPlots, sharex=True, squeeze=False)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "startTime = time.time()\n",
    "currentTime = startTime\n",
    "\n",
    "minimumRecordedTime = min(jobData.t.min(), tmData.t.min())\n",
    "referenceTime = int(startTime) if np.isnan(minimumRecordedTime) else minimumRecordedTime\n",
    "try:\n",
    "    while currentTime - startTime < ANALYSIS_DURATION_SEC:\n",
    "        # Retrieve metrics through Flink's REST API\n",
    "        jobData = recordVertexMetrics(jobData, taskMetrics, currentTime, TASK_METRIC_PATTERN)\n",
    "        jobData = recordVertexMetrics(jobData, operatorMetrics, currentTime, OPERATOR_METRIC_PATTERN)\n",
    "        tmData = recordTaskManagerMetrics(tmData, taskManagers, taskManagerMetrics, currentTime)\n",
    "        # Plot task and operator metrics\n",
    "        axesIndex = 0\n",
    "        for metric in taskMetricNames + operatorMetricNames:\n",
    "            ax = axes[axesIndex][0]\n",
    "            plotMetric(jobData, ax, referenceTime, metric, ['t', 'vertex', 'component'], ['vertex', 'component'])\n",
    "            axesIndex += 1\n",
    "        # Plot task manager metrics\n",
    "        for metric in taskManagerMetrics:\n",
    "            ax = axes[axesIndex][0]\n",
    "            plotMetric(tmData, ax, referenceTime, metric, ['t', 'tm', 'metric'], ['tm', 'metric'])\n",
    "            axesIndex += 1\n",
    "        fig.canvas.draw()\n",
    "        currentTime = time.time()\n",
    "        time.sleep(SAMPLING_FREQ_SEC)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
